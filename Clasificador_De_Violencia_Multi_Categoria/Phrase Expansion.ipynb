{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Text Data Augmentation\n","1. Word or sentence shuffling: randomly changing the position of a word or sentence.\n","2. Word replacement: replace words with synonyms.\n","3. Syntax-tree manipulation: paraphrase the sentence using the same word.\n","4. Random word insertion: inserts words at random.\n","5. Random word deletion: deletes words at random."],"metadata":{"id":"SbpGsNc25Ye5"}},{"cell_type":"markdown","source":["## Easy Data Augmentation"],"metadata":{"id":"unQrOe9H3YCI"}},{"cell_type":"markdown","source":["Pasos:\n","1. Synonym Replacement\n","2. Random Insertion\n","3. Random Swap\n","4. Random Deletion\n","\n","Repositorio:\n","https://github.com/jasonwei20/eda_nlp"],"metadata":{"id":"PKHkbHHl3Zvg"}},{"cell_type":"markdown","source":["Se deberia buscar una alternativa en español (como lo que esta arriba)"],"metadata":{"id":"MXaPItte3yPY"}},{"cell_type":"markdown","source":["## NLP Albumentation"],"metadata":{"id":"Tp3w-ZcV4E1q"}},{"cell_type":"markdown","source":["Shuffle Sentences Transform:\n","\n","text = ‘\\<Sentence1>. \\<Sentence2>. \\<Sentence4>. \\<Sentence4>. \\<Sentence5>. \\<Sentence5>.’\n","\n","pasa a ser:\n","\n","\n","text = ‘\\<Sentence2>. \\<Sentence3>. \\<Sentence1>. \\<Sentence5>. \\<Sentence5>. \\<Sentence4>.’"],"metadata":{"id":"nT9k8nQI4I3Z"}},{"cell_type":"markdown","source":["Exclude duplicate transform"],"metadata":{"id":"UGUhp9-P4wHj"}},{"cell_type":"markdown","source":[],"metadata":{"id":"VvSqTd0j4x2h"}},{"cell_type":"markdown","source":["## Back translation"],"metadata":{"id":"YBhaHIor2Rm5"}},{"cell_type":"markdown","source":["### notebook"],"metadata":{"id":"SrFFIyho6dhC"}},{"cell_type":"markdown","source":["codigo sacado de notebook: https://www.kaggle.com/code/miklgr500/how-to-use-translators-for-comments-translation/notebook\n","\n","Tiene que ser adaptado con un input y output propio"],"metadata":{"id":"8K0vGdoO3GZz"}},{"cell_type":"code","source":["!pip install translators\n","\n","import pandas as pd\n","# current version have logs, which is not very comfortable\n","import translators as ts\n","from multiprocessing import Pool\n","from tqdm import *\n","\n","CSV_PATH = ''\n","LANG = 'es'\n","API = 'google'\n","\n","\n","def translator_constructor(api):\n","    if api == 'google':\n","        return ts.google\n","    elif api == 'bing':\n","        return ts.bing\n","    elif api == 'baidu':\n","        return ts.baidu\n","    elif api == 'sogou':\n","        return ts.sogou\n","    elif api == 'youdao':\n","        return ts.youdao\n","    elif api == 'tencent':\n","        return ts.tencent\n","    elif api == 'alibaba':\n","        return ts.alibaba\n","    else:\n","        raise NotImplementedError(f'{api} translator is not realised!')\n","\n","\n","def translate(x):\n","    try:\n","        return [x[0], translator_constructor(API)(x[1], 'en', LANG), x[2]]\n","    except:\n","        return [x[0], None, [2]]\n","\n","\n","def imap_unordered_bar(func, args, n_processes: int = 48):\n","    p = Pool(n_processes, maxtasksperchild=100)\n","    res_list = []\n","    with tqdm(total=len(args)) as pbar:\n","        for i, res in tqdm(enumerate(p.imap_unordered(func, args))):\n","            pbar.update()\n","            res_list.append(res)\n","    pbar.close()\n","    p.close()\n","    p.join()\n","    return res_list\n","\n","\n","def main():\n","    df = pd.read_csv(CSV_PATH).sample(100)\n","    tqdm.pandas('Translation progress')\n","    df[['id', 'comment_text', 'toxic']] = imap_unordered_bar(translate, df[['id', 'comment_text', 'toxic']].values)\n","    df.to_csv(f'expansion-{API}-{LANG}.csv')\n","\n","\n","if __name__ == '__main__':\n","    main()"],"metadata":{"id":"hhJkAyJE2W5I","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1688927358976,"user_tz":180,"elapsed":9752,"user":{"displayName":"Pablo Buendia","userId":"01284267746608993755"}},"outputId":"2f6d336e-ecde-4a6c-8a43-ff31a76846c1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting translators\n","  Downloading translators-5.7.9-py3-none-any.whl (53 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/53.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting requests>=2.29.0 (from translators)\n","  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting PyExecJS>=1.5.1 (from translators)\n","  Downloading PyExecJS-1.5.1.tar.gz (13 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: lxml>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from translators) (4.9.2)\n","Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from translators) (4.65.0)\n","Collecting pathos>=0.2.9 (from translators)\n","  Downloading pathos-0.3.0-py3-none-any.whl (79 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting cryptography>=38.0.1 (from translators)\n","  Downloading cryptography-41.0.1-cp37-abi3-manylinux_2_28_x86_64.whl (4.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=38.0.1->translators) (1.15.1)\n","Collecting ppft>=1.7.6.6 (from pathos>=0.2.9->translators)\n","  Downloading ppft-1.7.6.6-py3-none-any.whl (52 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting dill>=0.3.6 (from pathos>=0.2.9->translators)\n","  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pox>=0.3.2 (from pathos>=0.2.9->translators)\n","  Downloading pox-0.3.2-py3-none-any.whl (29 kB)\n","Collecting multiprocess>=0.70.14 (from pathos>=0.2.9->translators)\n","  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from PyExecJS>=1.5.1->translators) (1.16.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.29.0->translators) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.29.0->translators) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.29.0->translators) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.29.0->translators) (2023.5.7)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=38.0.1->translators) (2.21)\n","Building wheels for collected packages: PyExecJS\n","  Building wheel for PyExecJS (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for PyExecJS: filename=PyExecJS-1.5.1-py3-none-any.whl size=14582 sha256=56f8bf3f6a49a342ff86a02c42437b0c67792979266be8f99abb51720e787b67\n","  Stored in directory: /root/.cache/pip/wheels/9d/91/30/28e6da53d4f44dc445349b2ffad581968447e4cbc9dd7991b8\n","Successfully built PyExecJS\n","Installing collected packages: requests, PyExecJS, ppft, pox, dill, multiprocess, cryptography, pathos, translators\n","  Attempting uninstall: requests\n","    Found existing installation: requests 2.27.1\n","    Uninstalling requests-2.27.1:\n","      Successfully uninstalled requests-2.27.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires requests==2.27.1, but you have requests 2.31.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed PyExecJS-1.5.1 cryptography-41.0.1 dill-0.3.6 multiprocess-0.70.14 pathos-0.3.0 pox-0.3.2 ppft-1.7.6.6 requests-2.31.0 translators-5.7.9\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["requests"]}}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Using region Oregon server backend.\n","\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-de08d855d693>\u001b[0m in \u001b[0;36m<cell line: 60>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-4-de08d855d693>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCSV_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Translation progress'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'comment_text'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'toxic'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimap_unordered_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'comment_text'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'toxic'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: ''"]}]},{"cell_type":"markdown","source":["## gpt"],"metadata":{"id":"Tw_qgc6o6iQa"}},{"cell_type":"markdown","source":["(esto no anda pero lo podriamos debuguear y  ver si funciona)"],"metadata":{"id":"xUWmfbDeDOeq"}},{"cell_type":"code","source":["!pip install pandas numpy nltk googletrans==3.1.0a0\n","!pip install -U nltk"],"metadata":{"id":"ZPDoHSbZ7Ydz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1689083505273,"user_tz":180,"elapsed":15077,"user":{"displayName":"Valentina Fernández","userId":"10854393793641186105"}},"outputId":"4d52b494-7b65-4627-ccb1-1169e12ddd2a"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.22.4)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Collecting googletrans==3.1.0a0\n","  Downloading googletrans-3.1.0a0.tar.gz (19 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting httpx==0.13.3 (from googletrans==3.1.0a0)\n","  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2023.5.7)\n","Collecting hstspreload (from httpx==0.13.3->googletrans==3.1.0a0)\n","  Downloading hstspreload-2023.1.1-py3-none-any.whl (1.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.3.0)\n","Collecting chardet==3.* (from httpx==0.13.3->googletrans==3.1.0a0)\n","  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting idna==2.* (from httpx==0.13.3->googletrans==3.1.0a0)\n","  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==3.1.0a0)\n","  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n","Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==3.1.0a0)\n","  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0)\n","  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0)\n","  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0)\n","  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n","Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0)\n","  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2022.7.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n","Building wheels for collected packages: googletrans\n","  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for googletrans: filename=googletrans-3.1.0a0-py3-none-any.whl size=16352 sha256=1ef0fc6480655295b91ebb068281e65f8ab53702ae7ea7118e781081d55b5ce6\n","  Stored in directory: /root/.cache/pip/wheels/50/5d/3c/8477d0af4ca2b8b1308812c09f1930863caeebc762fe265a95\n","Successfully built googletrans\n","Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n","  Attempting uninstall: chardet\n","    Found existing installation: chardet 4.0.0\n","    Uninstalling chardet-4.0.0:\n","      Successfully uninstalled chardet-4.0.0\n","  Attempting uninstall: idna\n","    Found existing installation: idna 3.4\n","    Uninstalling idna-3.4:\n","      Successfully uninstalled idna-3.4\n","Successfully installed chardet-3.0.4 googletrans-3.1.0a0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2023.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n"]}]},{"cell_type":"markdown","source":["#### opcion con traductor\n","(capaz como hay mucho lunfardo no es lo mejor, pero probamos)"],"metadata":{"id":"DkxIR8Zd718r"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import nltk\n","from nltk.corpus import wordnet\n","from nltk.tokenize import word_tokenize\n","from googletrans import Translator\n","\n","# Cargar el archivo de Excel con las frases originales\n","df = pd.read_excel('frases.xlsx')\n","\n","# Función para realizar la traducción de una frase a otro idioma\n","def translate_text(text, target_lang):\n","    translator = Translator(service_urls=['translate.google.com'])\n","    translation = translator.translate(text, dest=target_lang)\n","    return translation.text\n","\n","# Función para realizar la sinonimia de una palabra\n","def get_synonyms(word):\n","    synonyms = []\n","    for syn in wordnet.synsets(word):\n","        for lemma in syn.lemmas():\n","            synonyms.append(lemma.name())\n","    return synonyms\n","\n","# Función para realizar la mezcla de palabras en una frase\n","def mix_words(text):\n","    tokens = word_tokenize(text)\n","    mixed_text = ' '.join(np.random.permutation(tokens))\n","    return mixed_text\n","\n","# Lista para almacenar las frases aumentadas\n","augmented_data = []\n","\n","# Aumentación de datos utilizando traducción, sinonimia y mezcla de palabras\n","for index, row in df.iterrows():\n","    original_text = row['Frases']\n","    target_lang = 'en'  # Idioma objetivo para la traducción\n","\n","    # Traducción de la frase original a otro idioma\n","    translated_text = translate_text(original_text, target_lang)\n","    augmented_data.append(translated_text)\n","\n","    # Sinonimia de las palabras en la frase original\n","    tokens = word_tokenize(original_text)\n","    for token in tokens:\n","        synonyms = get_synonyms(token)\n","        if len(synonyms) > 0:\n","            random_synonym = np.random.choice(synonyms)\n","            augmented_text = original_text.replace(token, random_synonym)\n","            augmented_data.append(augmented_text)\n","\n","    # Mezcla de palabras en la frase original\n","    mixed_text = mix_words(original_text)\n","    augmented_data.append(mixed_text)\n","\n","# Crear un nuevo DataFrame con las frases aumentadas\n","augmented_df = pd.DataFrame(augmented_data, columns=['Frase'])\n","\n","# Guardar el DataFrame en un nuevo archivo de Excel\n","augmented_df.to_excel('frases_aumentadas.xlsx', index=False)\n"],"metadata":{"id":"cwjSLFmn6l1y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### opcion sin traductor"],"metadata":{"id":"0cMtLeKZ7_74"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import nltk\n","import random\n","from nltk.corpus import wordnet\n","from nltk.tokenize import word_tokenize\n","from random import shuffle\n","\n","# Descargar los recursos necesarios de NLTK\n","nltk.download('wordnet')\n","nltk.download('punkt')\n","nltk.download('omw-1.4')\n","\n","# Cargar el archivo de Excel con las frases originales\n","df = pd.read_excel('frases.xlsx')\n","\n","# Función para obtener sinónimos de una palabra en español\n","def get_synonyms(word):\n","    synonyms = []\n","    for syn in wordnet.synsets(word, lang='spa'):\n","        for lemma in syn.lemmas(lang='spa'):\n","            synonyms.append(lemma.name())\n","    return synonyms\n","\n","# Función para realizar la mezcla de palabras en una frase\n","def mix_words(text):\n","    tokens = word_tokenize(text, language='spanish')\n","    shuffle(tokens)\n","    mixed_text = ' '.join(tokens)\n","    return mixed_text\n","\n","# Lista para almacenar las frases aumentadas\n","augmented_data = []\n","\n","# Esto es para eliminar errores que provocan las frases vacias nulas\n","original_text = df['Frases'].apply(str)\n","\n","for text_sentence in original_text:\n","  # Aumentación de datos utilizando sinonimia y mezcla de palabras\n","  tokens = word_tokenize(text_sentence, language='spanish')\n","  for token in tokens:\n","    if token.lower() == 'de': # Esto porque los sinónimos para 'de' son cualquier cosa ['Delaware', 'Estado_Diamante', 'Primer_Estado']\n","      continue\n","    synonyms = get_synonyms(token)\n","    synonyms = [value for value in synonyms if value.lower() != token.lower()] # Esto para sacar los sinónimos que son solo la misma letra pero en mayúscula\n","    if len(synonyms) > 0:\n","      random_synonym = random.choice(synonyms) # Tambien se puede directamente usar todos los sinónimos y no uno random, para generar mas texto\n","      while random_synonym == token:\n","        random_synonym = np.random.choice(synonyms)\n","      print(\"Token: \", token, \", synonyms: \", synonyms, \" selected synonym \", random_synonym)\n","      augmented_text = text_sentence.replace(token, random_synonym)\n","      augmented_data.append(augmented_text)\n","\n","  # Mezcla de palabras en la frase original\n","  mixed_text = mix_words(text_sentence)\n","  augmented_data.append(mixed_text)\n","\n","# Crear un nuevo DataFrame con las frases aumentadas\n","augmented_df = pd.DataFrame(augmented_data, columns=['Frase'])\n","\n","# Guardar el DataFrame en un nuevo archivo de Excel\n","augmented_df.to_excel('frases_aumentadas.xlsx', index=False)\n"],"metadata":{"id":"06lqBCec8BQJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## BERT"],"metadata":{"id":"VkmmDIWO2K5J"}},{"cell_type":"code","source":["!pip install transformers"],"metadata":{"id":"GJE_oadFRL21","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688925603032,"user_tz":180,"elapsed":19843,"user":{"displayName":"Pablo Buendia","userId":"01284267746608993755"}},"outputId":"ebdafc20-8ae6-4f3d-c5dd-f1225d2f6ff0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n","  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m97.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.16.4 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vslZb6AsREhp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1688926018815,"user_tz":180,"elapsed":15259,"user":{"displayName":"Pablo Buendia","userId":"01284267746608993755"}},"outputId":"84704f12-330d-4dbe-b4a8-3921abb0171b"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['classifier.weight', 'bert.pooler.dense.weight', 'classifier.bias', 'bert.pooler.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Phrase: Sos una inútil y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Inutil, nadie te va a amar\n","Predicted Violence Type: Symbolic\n","\n","Phrase: Sos una inservible y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una incompetente y nadie te va a querer nunca.\n","Predicted Violence Type: Symbolic\n","\n","Phrase: Sos una despreciable y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una hija de puta y nadie te va a querer nunca.\n","Predicted Violence Type: Physical\n","\n","Phrase: Sos una idiota y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una hija de perra y nadie te va a querer nunca.\n","Predicted Violence Type: Physical\n","\n","Phrase: Sos una rata inmunda y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una fea y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una cerda y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una asquerosa y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una sucia y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una esquizofrémica y nadie te va a querer nunca.\n","Predicted Violence Type: Symbolic\n","\n","Phrase: Sos una inculta y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una analfabeta y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una culera y nadie te va a querer nunca.\n","Predicted Violence Type: Physical\n","\n","Phrase: Sos una gorda y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una puta y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una zorra y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una chismosa y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una mantenida y nadie te va a querer nunca.\n","Predicted Violence Type: Symbolic\n","\n","Phrase: Sos una vulgar y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una operada y nadie te va a querer nunca.\n","Predicted Violence Type: Psychological\n","\n","Phrase: Sos una enana y nadie te va a querer nunca.\n","Predicted Violence Type: Physical\n","\n","Phrase: Sos una hocico y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una psicópata y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una puerca y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una cínica y nadie te va a querer nunca.\n","Predicted Violence Type: Sexual\n","\n","Phrase: Sos una verdulera y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una mandona y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una fría y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una bruja y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una barata y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una mentirosa y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una entrometida y nadie te va a querer nunca.\n","Predicted Violence Type: Symbolic\n","\n","Phrase: Sos una escoria y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una estúpida y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una farsante y nadie te va a querer nunca.\n","Predicted Violence Type: Symbolic\n","\n","Phrase: Sos una ladrona y nadie te va a querer nunca.\n","Predicted Violence Type: Physical\n","\n","Phrase: Sos una corrupta y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una maldita y nadie te va a querer nunca.\n","Predicted Violence Type: Economic\n","\n","Phrase: Sos una grosera y nadie te va a querer nunca.\n","Predicted Violence Type: Physical\n","\n","Phrase: Sos una vieja y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una sonsa y nadie te va a querer nunca.\n","Predicted Violence Type: Symbolic\n","\n","Phrase: Sos una hipócrita y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una imbécil y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una obesa y nadie te va a querer nunca.\n","Predicted Violence Type: Physical\n","\n","Phrase: Sos una mongola y nadie te va a querer nunca.\n","Predicted Violence Type: Economic\n","\n","Phrase: Sos una mogólica y nadie te va a querer nunca.\n","Predicted Violence Type: Symbolic\n","\n","Phrase: Sos una mogolica y nadie te va a querer nunca.\n","Predicted Violence Type: Physical\n","\n","Phrase: Sos una mongolica y nadie te va a querer nunca.\n","Predicted Violence Type: Physical\n","\n","Phrase: Sos una perra y nadie te va a querer nunca.\n","Predicted Violence Type: Symbolic\n","\n","Phrase: Sos una tonta y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una flácida y nadie te va a querer nunca.\n","Predicted Violence Type: Symbolic\n","\n","Phrase: Sos una débil y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una indígena y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una chola y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una amargada y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una tarada y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una débil y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una cualquiera y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una perdida y nadie te va a querer nunca.\n","Predicted Violence Type: Sexual\n","\n","Phrase: Sos una adefesio y nadie te va a querer nunca.\n","Predicted Violence Type: Symbolic\n","\n","Phrase: Sos una arpía y nadie te va a querer nunca.\n","Predicted Violence Type: Symbolic\n","\n","Phrase: Sos una arrastrada y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una burra y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una babosa y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una bruta y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una bestia y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una escuálida y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una raquítica y nadie te va a querer nunca.\n","Predicted Violence Type: Symbolic\n","\n","Phrase: Sos una anoréxica y nadie te va a querer nunca.\n","Predicted Violence Type: Sexual\n","\n","Phrase: Sos una bulímica y nadie te va a querer nunca.\n","Predicted Violence Type: Psychological\n","\n","Phrase: Sos una enferma y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una borracha y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una lesbiana y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una alcohólica y nadie te va a querer nunca.\n","Predicted Violence Type: Symbolic\n","\n","Phrase: Sos una pendeja y nadie te va a querer nunca.\n","Predicted Violence Type: Physical\n","\n","Phrase: Sos una ridícula y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una negra y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una sidosa y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una insegura y nadie te va a querer nunca.\n","Predicted Violence Type: Symbolic\n","\n","Phrase: Sos una ramera y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una prostituta y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una reprimida y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una víbora y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una rata y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una impertinente y nadie te va a querer nunca.\n","Predicted Violence Type: Physical\n","\n","Phrase: Sos una impresentable y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una incapaz y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una inepta y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una infame y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una infeliz y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una ingrata y nadie te va a querer nunca.\n","Predicted Violence Type: Symbolic\n","\n","Phrase: Sos una desagradecida y nadie te va a querer nunca.\n","Predicted Violence Type: Symbolic\n","\n","Phrase: Sos una insensata y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una insolente y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una trastornada y nadie te va a querer nunca.\n","Predicted Violence Type: Physical\n","\n","Phrase: Sos una loca y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una sinverguenza y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una indeseable y nadie te va a querer nunca.\n","Predicted Violence Type: Physical\n","\n","Phrase: Sos una bipolar y nadie te va a querer nunca.\n","Predicted Violence Type: Symbolic\n","\n","Phrase: Sos una despechada y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una mojigata y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una ciega y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una inválida y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una vaga y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una cornuda y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una conchuda y nadie te va a querer nunca.\n","Predicted Violence Type: Physical\n","\n","Phrase: Sos una cretina y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una energúmena y nadie te va a querer nunca.\n","Predicted Violence Type: Symbolic\n","\n","Phrase: Sos una engendra y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una piojosa y nadie te va a querer nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: Sos una mugrienta y nadie te va a querer nunca.\n","Predicted Violence Type: Physical\n","\n","Phrase: Sos una inútil y nadie te va a amar nunca.\n","Predicted Violence Type: Symbolic\n","\n","Phrase: Sos una inútil y nadie te va a valorar nunca.\n","Predicted Violence Type: Symbolic\n","\n","Phrase: Sos una inútil y nadie te va a cuidar nunca.\n","Predicted Violence Type: Undefined\n","\n","Phrase: una y inútil va querer a Sos nunca. te nadie\n","Predicted Violence Type: Economic\n","\n"]}],"source":["from transformers import BertTokenizer, BertForSequenceClassification\n","import random\n","\n","def modify_sentence_structure(phrase):\n","    words = phrase.split()\n","    random.shuffle(words)\n","    return ' '.join(words)\n","\n","# Load the pre-trained BERT model and tokenizer for Spanish\n","model_name = 'dccuchile/bert-base-spanish-wwm-uncased'\n","tokenizer = BertTokenizer.from_pretrained(model_name)\n","model = BertForSequenceClassification.from_pretrained(model_name, num_labels=6)\n","\n","# Example seed phrase in Spanish\n","seed_phrase = \"Sos una inútil y nadie te va a querer nunca.\"\n","\n","# Example violence type (Psychological)\n","violence_type = \"Psychological\"\n","\n","# Generate phrase variations\n","phrase_variations = [\"Inutil, nadie te va a amar\"]\n","\n","# Replace keywords\n","keyword_replacements = {\n","    'inútil': ['inservible', 'incompetente', 'despreciable','hija de puta','idiota', 'hija de perra',\n","               'rata inmunda', 'fea', 'cerda', 'asquerosa', 'sucia', 'esquizofrémica', 'inculta', 'analfabeta',\n","               'culera', 'gorda', 'puta', 'zorra', 'chismosa', 'mantenida', 'vulgar', 'operada', 'enana',\n","               'hocico', 'psicópata','puerca','cínica','verdulera','mandona','fría','bruja','barata',\n","               'mentirosa','entrometida','escoria','estúpida','farsante','ladrona','corrupta','maldita',\n","               'grosera','vieja','sonsa','hipócrita','imbécil','obesa','mongola','mogólica','mogolica',\n","               'mongolica','perra','tonta','flácida','débil','indígena','chola','amargada','tarada',\n","               'débil','cualquiera','perdida','adefesio','arpía','arrastrada','burra','babosa','bruta',\n","               'bestia','escuálida','raquítica','anoréxica','bulímica','enferma','borracha','lesbiana',\n","               'alcohólica','pendeja','ridícula','negra','sidosa','insegura','ramera','prostituta',\n","               'reprimida','víbora','rata','impertinente','impresentable','incapaz','inepta','infame',\n","               'infeliz','ingrata','desagradecida','insensata','insolente','trastornada','loca',\n","               'sinverguenza','indeseable','bipolar','despechada','mojigata','ciega','inválida',\n","               'vaga','cornuda','conchuda','cretina','energúmena','engendra','piojosa','mugrienta'],\n","    'querer': ['amar', 'valorar','cuidar'],\n","}\n","\n","for keyword, replacements in keyword_replacements.items():\n","    for replacement in replacements:\n","        variation = seed_phrase.replace(keyword, replacement)\n","        phrase_variations.append(variation)\n","\n","# Modify sentence structure\n","modified_structure = modify_sentence_structure(seed_phrase)\n","phrase_variations.append(modified_structure)\n","\n","# Introduce additional context\n","additional_phrases = {\n","    'sexual': ['Tenes que hacer lo que yo quiera en la cama.', 'Tengo derecho sobre tu cuerpo.'],\n","    'física': ['Te voy a matar.', 'Te mereces ser golpeada.'],\n","    'económica': ['No vas a ver ni un peso mío.', 'No podes comprar nada sin mí.'],\n","    'psicológica': ['Sos demasiado débil para enfrentar la vida.', 'Nunca vas a tener éxito sin mí.'],\n","    'simbólica': ['Yo controlo todos tus movimientos.'],\n","    'indefinida': ['Pagarás por desafiarme.', 'No puedes escapar de mi ira.'],\n","}\n","\n","additional_context_phrases = additional_phrases.get(violence_type, [])\n","phrase_variations.extend(additional_context_phrases)\n","\n","# Combine variations with original dataset\n","dataset = [seed_phrase] + phrase_variations\n","\n","# Manual labeling and other steps (not included in this example)\n","\n","# Tokenize and prepare the dataset for model input\n","encoded_inputs = tokenizer(dataset, padding=True, truncation=True, return_tensors='pt')\n","\n","# Classify the phrases using the pre-trained BERT model\n","outputs = model(**encoded_inputs)\n","logits = outputs.logits\n","predictions = logits.argmax(dim=1)\n","\n","# Print the predicted violence types for the phrases\n","for phrase, prediction in zip(dataset, predictions):\n","    predicted_violence_type = [\"Sexual\", \"Physical\", \"Economic\", \"Psychological\", \"Symbolic\", \"Undefined\"][prediction.item()]\n","    print(f\"Phrase: {phrase}\")\n","    print(f\"Predicted Violence Type: {predicted_violence_type}\")\n","    print()\n"]},{"cell_type":"code","source":["\n","template.format_map(kwargs)\n","\n","\n","itertools.product(adjetivos,verbos)"],"metadata":{"id":"3xRxg0dMMFvq"},"execution_count":null,"outputs":[]}]}